\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{subcaption}
\usepackage{float}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4}    % how many sectioning levels to show in ToC
\begin{document}
\title{\textbf{CS6011: Kernel Methods for Pattern Analysis}
\\
\textbf{Programming Assignment 1}
}
\author{Aravind Sankar CS11B033 \\
Ramnandan SK CS11B061 \\
Adit Krishnan CS11B063 \\[0.2in]
Group 2
}
\floatplacement{figure}{H}
\maketitle
\tableofcontents 
\newpage
\section{Objective of the assignment}
The basic goal of this assignment is to explore the use of Bayes classifiers (GMM), Perceptron and Multi Layer Feed Forward Neural Networks (MLFFNN) for the task of classification, and the use of MLFFNN and Radial Basis Function Neural Networks (RBFNN) for the task of regression.

\section{Classification task}
\subsection{Dataset 1}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/linearlySeparable/input_data.png}
\caption{Given dataset}
\end{figure}

\subsubsection{Linearly Separable Dataset}
\begin{flushleft}
In this dataset, we are given a 4 class problem, where every pair of classes is linearly separable. We are required to compare the performance of 3 classifiers mentioned above.
The input data is as shown in Figure 1. \\[10pt]

\textbf{Bayes Classifier :}  \\[10pt]

A Bayes classifier requires the estimation class conditional probability densities, for each class. We use a Gaussian Mixture Model for this purpose. We have assumed that the prior is same for each of the classes. Hence, the posterior probability of class is same as the class conditional density at any point.


From the plot, it's obvious that one Gaussian is sufficient to model each of class conditional densities.
Each of the densities were obtained by performing EM algorithm. 


The contours of each of the Gaussians obtained are shown in Figure 2. We can observe that the Gaussians model the distribution of points of each class correctly.



\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/linearlySeparable/contours.png}
\caption{Given dataset}
\end{figure}

The results obtained are shown below in the form of confusion matrices :\\[10pt]

\textbf{Train} : \\[10pt]


\begin{tabular}{|c|c|}
\hline
150 & 0\\ 

\hline
0 & 600 \\ \hline

\end{tabular}\\[10pt]

\textbf{Validation}: \\[10pt]

\begin{tabular}{|c|c|} 
\hline
90 & 0\\ \hline
0 & 360 \\ \hline
\end{tabular}\\[10pt]

\textbf{Test} : \\[10pt]

\begin{tabular}{|c|c|} \hline 
60 & 0\\ \hline
0 & 240 \\ \hline 
 \end{tabular}\\[10pt]

We observe 100 \% accuracy in the classification of points in the validation and test data. The decision region plot, showing the separation between the 3 classes is shown in Figure 3.


\textbf{\\[10pt]Decision Region:\\[5pt]}


\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/linearlySeparable/decn_region_bayes.png}
\caption{Given dataset}
\end{figure}


\end{flushleft}


\begin{flushleft}
\textbf{Perceptron Classifier :}  \\[10pt]

Since the data is linearly separable across pairs of classes, we can use the Perceptron model to classify the points. 

One approach is to use the One-Vs-All method for classification.  For the we require each of the 4 sets (using one class positive and others negative) to be linearly separable. If we consider the One-Vs-All classifier for the Red-Class (where point of red class are positive), we cannot find a separating hyperplane.

So, we use the approach of One-Vs-One classification.  $ 4 \choose 2 $ = 6 classifiers are constructed by considering each pair of classes. For any new point x, we use the outputs of each of the classifiers, and take the majority vote. As expected, the performance of training data was 100 \% as the hyperplane obtained separates the training data.  \\[10pt]

\newpage
The results obtained are shown below  :\\[10pt]

\textbf{Train} : \\[10pt]


\begin{tabular}{|c|c|c|c|} \hline 250 & 0 & 0 & 0\\ \hline 0 & 250 & 0 & 0\\ \hline 0 & 0 & 250 & 0\\ \hline 0 & 0 & 0 & 250 \\ \hline \end{tabular} \\[10pt]

\textbf{Validation} : \\[10pt]

\begin{tabular}{|c|c|c|c|}  \hline 149 & 0 & 0 & 1\\ \hline 0 & 150 & 0 & 0\\ \hline 0 & 0 & 150 & 0\\ \hline 0 & 0 & 0 & 150 \\ \hline \end{tabular}  \\[10pt]


\textbf{Test}: \\[10pt]


\begin{tabular}{|c|c|c|c|}\hline 100 & 0 & 0 & 0\\ \hline 1 & 99 & 0 & 0\\ \hline 0 & 0 & 100 & 0\\ \hline 0 & 0 & 0 & 100 \\ \hline \end{tabular} \\[10pt]



From the results, we see that only one point is misclassified in the validation and test datasets, and that one point is a point in class 1 which is misclassified as class 2. This is because both the classes are very close to each other, and the hyperplanes obtained are not the optimal ones. \\[10pt]

The decision region plot obtained, showing the separation between the 3 classes is shown in Figure 4.
\newpage 

\textbf{\\[10pt]Decision Region:\\[5pt]}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/linearlySeparable/decn_region_perceptron.png}
\caption{}
\end{figure}
\newpage
\textbf{MLFFNN :} \\[10pt]

For this simple dataset, we use a feedforward neural network with 1 hidden layer. There are 4 nodes in the output layer, for which we use a 1-of-k representation of the class labels. The function used is the one available in the MATLAB \textit{nntools} toolbox. The MSE error function was used in the output layer.
The number of nodes in the hidden layer (model complexity) was estimated to be 9 with hyperbolic tangent function, using multiple iterations of cross-validation. For this configuration, the results obtained are shown below.
\\[5pt]
\includegraphics[scale=0.6]{Classification/linearlySeparable/net_config}
	
% Confusion matrices %
\textbf{\\[5pt]Confusion Matrices: \\[10pt]}
\textbf{Train :\\[5pt]}

\begin{tabular}{|c|c|c|c|} \hline 250 & 0 & 0 & 0\\ \hline 0 & 250 & 0 & 0\\ \hline 0 & 0 & 250 & 0\\ \hline 0 & 0 & 0 & 250 \\ \hline \end{tabular}


\textbf{\\[10pt]Validation :\\[5pt]}

\begin{tabular}{|c|c|c|c|c|} \hline  0 & 0 & 0 & 150\\ \hline 150 & 0 & 0 & 0\\ \hline 0 & 150 & 0 & 0\\ \hline  0 & 0 & 150 & 0\\ \hline 0 & 0 & 0 & 0 \\ \hline \end{tabular}

\textbf{\\[10pt]Test :\\[5pt]}


\begin{tabular}{|c|c|c|c|} \hline 100 & 0 & 0 & 0\\ \hline 0 & 100 & 0 & 0\\ \hline 0 & 0 & 100 & 0\\ \hline 0 & 0 & 0 & 100 \\ \hline \end{tabular} \\[10pt]


We can see that 100 \% accuracy is obtained on all the three sets. The neural network adequately models the decision boundaries between each class, as shown below in the decision region plot.

\newpage

\textbf{\\[10pt]Decision Region:\\[5pt]}
\includegraphics[scale=1]{Classification/linearlySeparable/decn_region_nn}

\textbf{\\[10pt]Output Layer Plots across Epochs of training}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/1_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/1_2}
  
\end{subfigure}
\caption{Epoch 1 class 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/1_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/1_4}
  
\end{subfigure}
\caption{Epoch 1 class 3 and 4}
\end{figure}

%\begin{figure}
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/2_1}
% 
%\end{subfigure}%
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/2_2}
%  
%\end{subfigure}
%\caption{Epoch 2 class 1 and 2}
%\end{figure}
%
%\begin{figure}
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/2_3}
% 
%\end{subfigure}%
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/2_4}
%  
%\end{subfigure}
%\caption{Epoch 2 class 3 and 4}
%\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/5_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/5_2}
  
\end{subfigure}
\caption{Epoch 5 class 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/5_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/5_4}
  
\end{subfigure}
\caption{Epoch 5 class 3 and 4}
\end{figure}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/20_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/20_2}
  
\end{subfigure}
\caption{Epoch 20 class 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/20_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/20_4}
  
\end{subfigure}
\caption{Epoch 20 class 3 and 4}
\end{figure}
Clearly, a stable output has been established by the 20th epoch, looking at the above plots. We also proceed to look at what the hidden layer is outputing at some epochs. \\[10pt]
\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_1}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_2}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_3}
  
\end{subfigure}
\caption{Epoch 1 hidden nodes 1,2,3}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_4}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_5}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_6}
  
\end{subfigure}
\caption{Epoch 1 hidden nodes 4,5,6}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_7}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_8}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h1_9}
  
\end{subfigure}
\caption{Epoch 1 hidden nodes 7,8,9}
\end{figure}

\newpage
\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_1}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_2}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_3}
  
\end{subfigure}
\caption{Epoch 5 hidden nodes 1,2,3}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_4}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_5}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_6}
  
\end{subfigure}
\caption{Epoch 5 hidden nodes 4,5,6}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_7}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_8}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h5_9}
  
\end{subfigure}
\caption{Epoch 5 hidden nodes 7,8,9}
\end{figure}
\newpage
\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_1}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_2}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_3}
  
\end{subfigure}
\caption{Epoch 20 hidden nodes 1,2,3}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_4}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_5}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_6}
  
\end{subfigure}
\caption{Epoch 20 hidden nodes 4,5,6}
\end{figure}

\begin{figure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_7}
 
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_8}
  
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/linearlySeparable/h20_9}
  
\end{subfigure}
\caption{Epoch 20 hidden nodes 7,8,9}
\end{figure}
We can notice the clear S shaped curves that appear out of the hidden layer in these plots. They appear steep because we have plotted the full range on the x and y axis. The beta values are tuned by the tool. Also note that the difference between epochs 5 and 20 is less than that between 1 and 5 as expected.\\[10pt]


\textbf{Observation :}
\begin{itemize}
\item As the given dataset is linearly separable, the bayes classifier is able model the linear decision boundaries well, giving 100\% accuracy.
\item The perceptron algorithm identifies separating hyperplanes between each pair classes, which may not be optimal. Because of this reason, we observe that the accuracy on validation and test data is less than 100 \%. Obviously, as the perceptron is able separate the training data, the training accuracy is 100 
\%
\item The neural network classifier, being a generalization on the basic perceptron classifier, is able to  model the decision boundaries well, giving 100 \% accuracy.
\end{itemize}
\newpage
\end{flushleft}

\subsubsection{Non-linearly Separable Dataset}
\begin{flushleft}

In this dataset, we are given a 2 class problem, which are non-linearly separable, i.e the 2 classes can be separated by a non-linear decision boundary. Here, we are required to compare the performance of Bayes classifier and MLFFNN.
The input data is as shown in the figure below. \\[10pt]

\end{flushleft}

\begin{figure}[H]

\centering
\includegraphics[width=\linewidth]{Classification/nonlinearlySeparable/input_data.png}
\caption{Given dataset}
\end{figure}



\begin{flushleft}

\textbf{Bayes classifier :}  \\[10pt]
The Gaussian mixture model was used to model the class conditional densities of each class. Unlike the previous data, we can see the need to use more than one component in the GMM used to model each of the classes. Hence, we need to choose the appropriate number of Gaussian components to represent each class.
\\[10pt]

For this purpose, we adopt an unsupervised approach. We use the Bayesian Information Criterion (BIC), which provides a trade-off between the likelihood of the data and the model complexity.

 $BIC = -2 \cdot \ln\hat L + k \cdot \ln(n)$  


where $\hat L$ is the maximized likelihood, k is the no. of parameters, and n the number of data points. The model with the lowest BIC value is expected to be the best one.


Based on this criterion, we obtained 3 Gaussian components for class 1 (inside region) and 5 Gaussian components for class 2. The Gaussian contours obtained for each of the classes are shown in the figures below.



\end{flushleft}


\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/nonlinearlySeparable/gmm_1.png}
\caption{Gaussian contours for class 1}
\end{figure}



\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/nonlinearlySeparable/gmm_2.png}
\caption{Gaussian contours for class 2}
\end{figure}

\begin{flushleft}

From these contours, we can validate the appropriateness of the metric used, as the gaussian components accurately model the inter-class variability.

The results obtained are shown below : \\[10pt]

\textbf{Train} : \\[10pt]

\begin{tabular}{|c|c|}  \hline 150 & 0\\ \hline 0 & 600 \\ \hline \end{tabular} \\[10pt]

\textbf{Validation} : \\[10pt]

\begin{tabular}{|c|c|} \hline 90 & 0\\ \hline 0 & 360 \\ \hline \end{tabular} \\[10pt]


\textbf{Test} : \\[10pt]
\begin{tabular}{|c|c|}  \hline 60  & 0\\ \hline 0 & 240 \\ \hline \end{tabular} \\[10pt]

\begin{flushleft}
As expected, since the Gaussian adequately model the distributions of the 2 nonlinearly separable classes, we get 100 \% accuracy on all 3 sets.

The decision region plot separating the 2 classes is shown below :
\newpage
\end{flushleft}

\textbf{Decision region : } \\[5pt]

\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/nonlinearlySeparable/decn_region_bayes.png}
\caption{Decision region plot}
\end{figure}



\begin{flushleft}
\textbf{MLFFNN} \\[10pt]
Here again, as in the previous dataset, we use a neural network with 1 hidden layer, for which the number of nodes is estimated using cross validation on the given validation dataset.

For the best performance by cross validation, we get 4 nodes in the hidden layer. For this configuration, the results obtained are shown below.
\\[5pt]
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{Classification/nonlinearlySeparable/net_config} 
\end{figure}
\newpage

\textbf{Decision Region :\\[5pt]}
\includegraphics[scale=1]{Classification/nonlinearlySeparable/decn_region_nn_nls}
\end{flushleft}
% Confusion matrices %
% Confusion matrices %
\begin{flushleft}
\textbf{\\[5pt]Confusion Matrices: \\[10pt]}
\textbf{Train :\\[5pt]}
\begin{tabular}{|c|c|}\hline  150 & 0 \\ \hline 0 & 600 \\ \hline \end{tabular}

\textbf{\\[10pt]Validation :\\[5pt]}

\begin{tabular}{|c|c|}\hline  90 & 0 \\ \hline 0 & 360 \\ \hline \end{tabular}

\textbf{\\[10pt]Test :\\[5pt]}


\begin{tabular}{|c|c|}\hline  60 & 0 \\ \hline 0 & 240 \\ \hline \end{tabular}

Here again, we see that the neural network is able to obtain 100 \% accuracy on all the three sets.
\end{flushleft}
\newpage
\textbf{Output Layer values with Epochs\\[5pt]}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/1_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/1_2}
  
\end{subfigure}
\caption{Epoch 1 class 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/5_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/5_2}
  
\end{subfigure}
\caption{Epoch 5 class 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/10_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/10_2}
  
\end{subfigure}
\caption{Epoch 10 class 1 and 2}
\end{figure}
Clearly, the outputs have converged to a great extent by the 10th epoch.
\newpage

\textbf{Hidden layer values with Epochs \\[10pt]}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h1_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h1_2}
\caption{Epoch 1 hidden nodes 1 and 2}
\end{subfigure}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h1_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h1_4}
  \end{subfigure}
\caption{Epoch 1 hidden nodes 3 and 4}
\end{figure}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h5_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h5_2}
  \end{subfigure}
\caption{Epoch 5 hidden nodes 1 and 2}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h5_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h5_4}
  \end{subfigure}
\caption{Epoch 5 hidden nodes 3 and 4}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h20_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h20_2}
  \end{subfigure}
\caption{Epoch 20 hidden nodes 1 and 2}

\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h20_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/nonlinearlySeparable/h20_4}
  \end{subfigure}
\caption{Epoch 20 hidden nodes 3 and 4}
\end{figure}
\end{figure}
The above shows a clean and smooth S shaped hyperbolic tangent curve at all epochs for the hidden layers. We chose hyperbolic tangent since the training was faster.

\textbf{Observations :} 
\begin{itemize}
\item The bayes classifier with GMM's models the class conditional densities of each class accurately, giving 100\% accuracy.
\item The neural network also models the non-linear decision boundary correctly, leading to 100 \% accuracy.

\end{itemize}
\newpage
\subsubsection{Overlapping classes Dataset}
\begin{flushleft}
In this dataset, we are given a 2 class problem, where every pair of classes is linearly non separable, i.e the most of the points can be separated by a linear decision boundary but a few points of each class lie on either side of the boundary. Here, we are required to compare the performance of Bayes classifier and MLFFNN.

The input data is as shown in the figure below. \\[10pt]
\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/overlapping/input_data.png}
\caption{Given dataset}
\end{figure}


\begin{flushleft}

\textbf{Bayes Classifier :} \\[10pt]


The Gaussian mixture model was used to model the class conditional densities of each class. In this dataset, we observe that using only Gaussian is sufficient to model the points in each class. This hypothesis was however verified using the BIC criterion. The gaussian contours obtained for the classes are shown in the figure below.


\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/overlapping/gmm.png}
\caption{Gaussian contours of the 4 classes}
\end{figure}

\begin{flushleft}

The results obtained are shown below : \\[10pt]

\textbf{Train} : \\[10pt]
\begin{tabular}{|c|c|c|c|} \hline 216 & 7 & 19 & 8\\ \hline 9 & 234 & 4 & 3\\ \hline 24 & 1 & 225 & 0\\ \hline 2 & 4 & 0 & 244 \\ \hline \end{tabular} \\[10pt]


\textbf{Validation} : \\[10pt]

\begin{tabular}{|c|c|c|c|} \hline 134 & 2 & 6 & 8\\ \hline 7 & 139 & 2 & 2\\ \hline 12 & 1 & 137 & 0\\ \hline  4 & 5 & 0 & 141 \\ \hline \end{tabular} \\[10pt]




\textbf{Test}: \\[10pt]


\begin{tabular}{|c|c|c|c|} \hline 90 & 1 & 7 & 2\\ \hline 7 & 88 & 4 & 1\\ \hline 14 & 3 & 83 & 0\\ \hline 2 & 0 & 0 & 98  \\ \hline \end{tabular} \\[10pt]



The decision region plot obtained is shown below :

\end{flushleft}

\textbf{Decision region : \\[5pt]}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/overlapping/decn_region_bayes.png}
\caption{Decision region separating the 4 classes}
\end{figure}



\begin{flushleft}
\textbf{MLFFNN} \\[10pt]
Here again, as in the previous dataset, we use a neural network with 1 hidden layer, for which the number of nodes is estimated using cross validation on the given validation dataset.

For the best performance on the validation data, we get 16 nodes in the hidden layer. For this configuration, the results obtained are shown below.
\\[5pt]
\begin{figure}
\centering
\includegraphics[scale=0.6]{Classification/overlapping/net_config}
\end{figure}
\newpage
\textbf{\\[5pt]Decision Region\\[10pt]}
\includegraphics[scale=1]{Classification/overlapping/decn_region_nn}
\end{flushleft}
% Confusion matrices %
\begin{flushleft}
\textbf{\\[5pt]Confusion Matrices: \\[10pt]}
\textbf{Train :\\[5pt]}
\begin{tabular}{|c|c|c|c|}\hline  215 & 8 & 19 & 8 \\ \hline 10 & 235 & 2& 3 \\ \hline 24 & 1 & 225& 0 \\ \hline 2 & 2 & 0& 246 \\ \hline \end{tabular}

\textbf{\\[10pt]Validation :\\[5pt]}

\begin{tabular}{|c|c|c|c|}\hline  132 & 2 & 7 & 9 \\ \hline 7 & 137 & 3& 3 \\ \hline 13 & 1 & 136& 0 \\ \hline 4 & 2 & 0& 144 \\ \hline \end{tabular}

\textbf{\\[10pt]Test :\\[5pt]}

\begin{tabular}{|c|c|c|c|}\hline  88 & 1 & 7 & 4 \\ \hline 6 & 89 & 4 & 1 \\ \hline 16 & 2 & 82& 0 \\ \hline 3 & 0 & 0& 97 \\ \hline \end{tabular}
\end{flushleft}

\newpage
\textbf{Output layer with Epochs \\[10pt]}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/1_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/1_2}
  \end{subfigure}
\caption{Epoch 1 output nodes 1 and 2}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/1_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/1_4}
  \end{subfigure}
\caption{Epoch 1 output nodes 3 and 4}
\end{figure}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/5_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/5_2}
  \end{subfigure}
\caption{Epoch 5 output nodes 1 and 2}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/5_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/5_4}
  \end{subfigure}
\caption{Epoch 5 output nodes 3 and 4}
\end{figure}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/20_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/20_2}
  \end{subfigure}
\caption{Epoch 20 output nodes 1 and 2}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/20_3}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/20_4}
  \end{subfigure}
\caption{Epoch 20 output nodes 3 and 4}
\end{figure}
\newpage





\textbf{Hidden Layer Values with Epochs \\[10pt]}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h1_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h1_5}
  \end{subfigure}
\caption{Epoch 1 hidden nodes 1 and 5}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h1_9}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h1_16}
  \end{subfigure}
\caption{Epoch 1 hidden nodes 9 and 16}
\end{figure}


\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h5_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h5_5}
  \end{subfigure}
\caption{Epoch 5 hidden nodes 1 and 5}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h5_9}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h5_16}
  \end{subfigure}
\caption{Epoch 5 hidden nodes 9 and 16}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h20_1}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h20_5}
  \end{subfigure}
\caption{Epoch 20 hidden nodes 1 and 5}
\end{figure}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h20_9}
 
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Classification/overlapping/h20_16}
  \end{subfigure}
\caption{Epoch 20 hidden nodes 9 and 16}
\end{figure}

\textbf{Observations :}
\begin{itemize}
\item For the bayes classifier, we get the test accuracy to be 89.75 \%. This is because the points in each pairs of classes overlap, which means it's impossible for any classifier to get 100\% accuracy.
\item Correspondingly, the test accuracy obtained by neural network model is 89 \%, which is almost the same as Bayes classifier.
\item We see that the loss in accuracy is not much, given the overlap of the points of the classes.
\end{itemize}

\newpage

\subsection{Dataset 2 - Image dataset}
In this dataset, we are given a set of images and are required to classify them according to the labels, which are  :


\begin{enumerate}

\item    light-house

\item    treadmill

\item    ladder

\item    leopards

\item    faces-easy


\end{enumerate}

The features extracted from the image are Color Histogram Features which give rise to 16 $\times$ 3 = 48 features.  We are required to compare the Bayes classifier and neural network classifier on this dataset.


\begin{flushleft}

\textbf{Bayes classifier :} \\[10pt]

In this case, as the number of dimensions is high, we chose to model the gaussians in the  mixture model with a diagonal covariance matrix. This was because the number of parameters needed to be estimated for each gaussian would be at least 2350, which much larger than the number of training examples we have for each class. We also use a small regularization parameter which is added to the covariance matrix to prevent singularity, as the features present were correlated. \\[10pt]


To find the number of gaussian components to use in the class conditional densities of each class, we used the performance (MSE) on the validation data to estimate the best number of components. The plot showing the variation of mis-classification error with the number of gaussian components is shown below :

\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/images/MisClassFraction.png}
\caption{Plot to find number of gaussian components}
\end{figure}
\begin{flushleft}

Here, we get 5 mixture components for each gaussian. 

The performance results obtained on splitting the data into train, validation and test are shown below :

\textbf{Train} : \\[10pt]


\begin{tabular}{|c|c|c|c|c|} \hline 54 & 20 & 35 & 0 & 5\\ \hline 4 & 58 & 12 & 1 & 13\\ \hline 2 & 32 & 88 & 0 & 20\\ \hline 0 & 0 & 0 & 114 & 0\\ \hline 0 & 4 & 3 & 0 & 254 \\ \hline \end{tabular}\\[10pt]

Train Accuracy : 79 \% \\[10pt]

\textbf{Validation} :  \\[10pt]
\begin{tabular}{|c|c|c|c|c|} \hline 11 & 7 & 14 & 0 & 7\\ \hline 2 & 21 & 4 & 0 & 2\\ \hline 5 & 12 & 23 & 0 & 8\\ \hline 0 & 0 & 0 & 39 & 0\\ \hline 1 & 9 & 3 & 0 & 74 \\ \hline \end{tabular}\\[10pt]

Validation Accuracy : 69.42  \% \\[10pt]
\textbf{Test} :  \\[10pt]
\begin{tabular}{|c|c|c|c|c|}  \hline 9 & 7 & 17 & 0 & 4\\ \hline 2 & 19 & 7 & 0 & 2\\ \hline 9 & 13 & 17 & 0 & 9\\ \hline 0 & 0 & 0 & 37 & 0\\ \hline 0 & 3 & 2 & 0 & 82 \\ \hline \end{tabular}\\[10pt]

Test Accuracy : 68.62 \% \\[10pt]


We observe that the accuracy obtained here, is quite less. One of the ways this can be improved is by considering different number of gaussian mixture components for each class. \\[10pt]


\textbf{MLFFNN :} \\[10pt]
\begin{flushleft}

For this dataset, as the complexity is high, we use a feed-forward neural network with 2 layers.  The number of nodes in these 2 layers represent the model complexity. The best complexity was found out using cross validation on the validation dataset. 

The best model configuration obtained is shown below :

\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/images/net_config.png}
\caption{Confusion matrices}
\end{figure}




The results obtained on using the best configuration are shown below in the form of confusion matrices :


\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Classification/images/confusion.png}
\caption{Confusion matrices}
\end{figure}

\textbf{Observations :}
\begin{itemize}
\item The test accuracy obtained by the bayes classifier (for the optimal no. of components) is 68.62 \%.
\item The neural network performs better than the bayes classifier and gives a test accuracy of 69.3 \%. 
\end{itemize}


\end{flushleft}
\newpage



















\section{Regression task}

In this task, we are given the same 2 datasets as the first assignment. In this assignment, we are required to use the MLFFNN and RBFNN models for the purpose of function approximation, specifically curve fitting and surface fitting in datasets 1 and 2 respectively.


\subsection{Dataset 1 (Univariate)}
\begin{flushleft}
Here, we are required to do curve fitting of the function $e^{cos(2 \pi x)}$. For this purpose we use a training set of 1000 points.



\end{flushleft}


\begin{flushleft}

\textbf{MLFFNN :}
For the uni-variate dataset, we use a feed-forward network with 1 hidden layer. The number of nodes in the hidden layer represents the model complexity, which is estimated by looking at the performance on the validation dataset. We varies the number of hidden layer nodes from 1 to 200, and compared the performance   (MSE) on all the three sets. The plot showing the variation of MSE w.r.t model complexity is shown below :

\end{flushleft}


\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Regression/univariate/mse.png}
\caption{Legend: Blue-Train  Green-Validation R-Test}
\end{figure}

From the above plot, we can see that the train error decreases on increasing the model complexity. This is expected, as a high number of nodes in the hidden layer leads to overfitting of the training data. So, the best neural network configuration is chosen as the one which gives least MSE on the validation data.
The configuration obtained is shown below, with 4 nodes in the hidden layer.


\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{Regression/univariate/net_config.png}
\caption{Neural network configuration}
\end{figure}

For this configuration, the plots of model and target outputs are shown in the figures below : 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/univariate/trainOutput.png}
\caption{Plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/univariate/valOutput.png}
\caption{Plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/univariate/testOutput.png}
\caption{Plots of model and target output (Test data)}
\end{figure}

\begin{flushleft}
From the plots, we can see that the model output closely follows the target output in all of the three plots, which shows the generalization ability of the neural network model.

Scatter plot with target output on $x$-axis and model output on $y$-axis for train, validation, test data is as follows:

\end{flushleft}

\begin{figure}[H]
\centering
\includegraphics[width=1.2\linewidth]{Regression/univariate/Scatter_plots.png}
\caption{Scatter plots of model and target output}
\end{figure}


\begin{flushleft}
Again from the scatter plots shown above, we can validate the correctness of the model obtained by observing that the scatter plot follows the $y = x$ line closely. 
\end{flushleft}
\newpage

\textbf{Output of output node after different number of epochs:}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{Regression/univariate/epoch_output.png}
\caption{Output of output nodes for different epochs}
\end{figure}

\begin{flushleft}

Here, we see that the neural network model starts with bad fit to the training data, and the model output progressively becomes closer to the true function (curve), as the number of epochs increase. The epoch number 1000 just represents the end of training phase. This is also expected as the neural network's performance improves over the epochs till the error function's value becomes nearly zero. 
\end{flushleft}

\newpage
\textbf{Output of hidden layer nodes for different number of epochs: \\[10pt]}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/univariate/hiddenLayer_1.png}\
  \caption{Epoch 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/univariate/hiddenLayer_2.png}
   \caption{Epoch 2}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/univariate/hiddenLayer_4.png}\
  \caption{Epoch 4}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/univariate/hiddenLayer_8.png}
   \caption{Epoch 8}
  \end{subfigure}
  
  \begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{Regression/univariate/hiddenLayer_1000.png}
   \caption{End of training}
  \end{subfigure}
  
\caption{Output of hidden layer nodes}
\end{figure}


\textbf{Observations :}

\begin{itemize}
\item In general, it was observed that neural network model was decently accurate in the fitting of a non-linear curve. 
\item The model output closely followed the target output in general. 
\item The values of MSE on train, val and test obtained for the best model were 0.0096, 0.0106 and 0.0112. \item These indicate that the model not only fits the train data properly, but also generalizes very well.
\end{itemize}


\textbf{Generalized RBF model :}

For the generalized Radial Basis Function Network, the model complexity depends on the number of prototypes chosen and regularization parameter. We chose the prototypes by performing k-means clustering and choosing the custer centers as prototypes. We varies the number of prototypes from 1 to 200 and regularization parameter from $10^{-6}$ to $10^4$, and compared the performance (MSE) on all the three sets. The plots showing the variation of MSE w.r.t model complexity and regularization parameter (each for optimal value of other parameter) are shown below :

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Regression/rbfnn_univariate/mse_lambda.png}
\caption{MSE vs Model Complexity (Regularization parameter)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Regression/rbfnn_univariate/mse_m.png}
\caption{MSE vs Model Complexity (Number of prototypes)}
\end{figure}

The optimal number of prototypes obtained was 10 and optimal regularization parameter was 0.01. The plots of model and target output for this are:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/output_train.png}
\caption{Plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/output_val.png}
\caption{Plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/output_test.png}
\caption{Plots of model and target output (Test data)}
\end{figure}

Scatter plot with target output on $x$-axis and model output on $y$-axis for train, validation, test data is as follows:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/scatter_train.png}
\caption{Scatter plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/scatter_val.png}
\caption{Scatter plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_univariate/scatter_test.png}
\caption{Scatter plots of model and target output (Test data)}
\end{figure}











\subsection{Dataset 2 (Bivariate)}

\textbf{MLFFNN with 1 hidden layer:}

\begin{flushleft}
For the bi-variate dataset, we first use a feed-forward network with 1 hidden layer. The number of nodes in the hidden layer represents the model complexity, which is estimated by looking at the performance on the validation dataset. We varies the number of hidden layer nodes from 1 to 200, and compared the performance (MSE) on all the three sets. The plot showing the variation of MSE w.r.t model complexity is shown below :

\end{flushleft}


\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Regression/bivariate/1layer_mse.png}
\caption{MSE vs Model Complexity}
\end{figure}

Here again, we observe similar trends as the previous case and the best configuration is obtained by choosing the model that gives the best validation performance.
The best neural network configuration obtained is shown below, with 8 nodes in the hidden layer.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{Regression/bivariate/net_config.png}
\caption{Neural network configuration}
\end{figure}

For this configuration, the plots of model and target outputs are shown in the figures below : 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_1layer_train.png}
\caption{Plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_1layer_val.png}
\caption{Plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_1layer_test.png}
\caption{Plots of model and target output (Test data)}
\end{figure}

\begin{flushleft}
In this dataset, the true surface is bowl shaped.  We can see that in all of the three plots, the model output also closely resembles this surface and closely follows the target output also. This can be verified by looking the scatter plots obtained below :
\end{flushleft}

Scatter plot with target output on $x$-axis and model output on $y$-axis for train, validation, test data is as follows:

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Regression/bivariate/scatter_1layer.png}
\caption{Scatter plots of model and target output}
\end{figure}

Here, in the scatter plots obtained we observe a $y = x$ for both train and validation datasets. But, for the test data, the best fit line deviates slightly from the $y = x$ line. This is because of slight overfitting which is present, even though the model with the complexity is chosen based on validation. 


\textbf{Output of output node after different number of epochs: } \\[10pt] 

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/1layer_epoch_1.png}\
  \caption{Epoch 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/1layer_epoch_2.png}
   \caption{Epoch 2}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/1layer_epoch_5.png}\
  \caption{Epoch 5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/1layer_epoch_15.png}
   \caption{Epoch 15}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/1layer_epoch_1000.png}\
  \caption{End of training}
\end{subfigure}%
  
\caption{Output of output layer node}
\end{figure}

\newpage

\textbf{Output of hidden layer node (one of the nodes) after different number of epochs:}

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_1layer_1.png}\
  \caption{Epoch 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_1layer_2.png}
   \caption{Epoch 2}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_1layer_5.png}\
  \caption{Epoch 5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_1layer_15.png}
   \caption{Epoch 15}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_1layer_1000.png}\
  \caption{End of training}
\end{subfigure}%
  
\caption{Output of hidden layer node}
\end{figure}










\textbf{MLFFNN with 2 hidden layers:}

For the bi-variate dataset, we first use a feed-forward network with 2 hidden layers. The number of nodes in each hidden layer represents the model complexity, which is estimated by looking at the performance on the validation dataset. We varies the number of hidden layer nodes from 1 to 200 (ensuring that the second layer has lesser nodes than the first), and compared the performance (MSE) on all the three sets. 

The best neural network configuration obtained is shown below, with 15 nodes in the first hidden layer and 10 in the second hidden layer.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{Regression/bivariate/net_config_2.png}
\caption{Neural network configuration}
\end{figure}

For this configuration, the plots of model and target outputs are shown in the figures below : 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_2layer_train.png}
\caption{Plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_2layer_val.png}
\caption{Plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/bivariate/output_2layer_test.png}
\caption{Plots of model and target output (Test data)}
\end{figure}

\begin{flushleft}

Here again, as in the earlier case the model outputs closely resemble the true surface of data to be fut, which is bowl shaped.
Scatter plot with target output on $x$-axis and model output on $y$-axis for train, validation, test data is as follows:
\end{flushleft}


\begin{figure}[H]
\centering
\includegraphics[width=1.2\linewidth]{Regression/bivariate/scatter_2layers.png}
\caption{Scatter plots of model and target output}
\end{figure}

Here, we observe very slight deviations from the $y = x$ line for the test and validation scatter plots. This is because of the slight overfitting caused by using 2 layers in the neural network.
The plot of output of output node after different number of epochs:

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/2layers_epoch_1.png}\
  \caption{Epoch 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/2layers_epoch_2.png}
   \caption{Epoch 2}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/2layers_epoch_5.png}\
  \caption{Epoch 5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/2layers_epoch_15.png}
   \caption{Epoch 15}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/2layers_epoch_1000.png}\
  \caption{End of training}
\end{subfigure}%
  
\caption{Output of output layer node}
\end{figure}

The plot of output of hidden layer 1 node (one of the nodes) after different number of epochs:

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_2layer_1.png}\
  \caption{Epoch 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_2layer_2.png}
   \caption{Epoch 2}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_2layer_5.png}\
  \caption{Epoch 5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_2layer_15.png}
   \caption{Epoch 15}
  \end{subfigure}
  
  \begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Regression/bivariate/hidden_2layer_1000.png}\
  \caption{End of training}
\end{subfigure}%
  
\caption{Output of output layer node}
\end{figure}

\textbf{Observations :}
\begin{itemize}
\item Both the models of neural networks, i.e. with 1 and 2 hidden layers seemed to perform well in the surface fitting task.
\item The non-linearities in the surface to be approximated are captured by using the tanh sigmoidal function in the hidden layer nodes. 
\item Though very slight overfitting is observed, the models obtained have good generalization ability.  

\end{itemize}


\textbf{Generalized RBF Model:}

For the generalized Radial Basis Function Network, the model complexity depends on the number of prototypes chosen and regularization parameter. We chose the prototypes by performing k-means clustering and choosing the cluster centers as prototypes. We varies the number of prototypes from 1 to 500 and regularization parameter from $10^{-6}$ to $10^4$, and compared the performance (MSE) on all the three sets. The plots showing the variation of MSE w.r.t model complexity and regularization parameter (each for optimal value of other parameter) are shown below :

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Regression/rbfnn_bivariate/mse_lambda.png}
\caption{MSE vs Model Complexity (Regularization parameter)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{Regression/rbfnn_bivariate/mse_m.png}
\caption{MSE vs Model Complexity (Number of prototypes)}
\end{figure}

The plots of model and target output for train, validation and test data are:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/output_train.png}
\caption{Plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/output_val.png}
\caption{Plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/output_test.png}
\caption{Plots of model and target output (Test data)}
\end{figure}

Scatter plot with target output on $x$-axis and model output on $y$-axis for train, validation, test data is as follows:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/scatter_train.png}
\caption{Scatter plots of model and target output (Train data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/scatter_val.png}
\caption{Scatter plots of model and target output (Validation data)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{Regression/rbfnn_bivariate/scatter_test.png}
\caption{Scatter plots of model and target output (Test data)}
\end{figure}



%\begin{figure}[H]
%
%\begin{subfigure}{.5\textwidth}
%\centering
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N1000M1000lambda1e-06}
%\caption{$\lambda$ = $10^{-6}$}
%\end{subfigure}
%\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N1000M1000lambda0_01}
%\caption{$\lambda$ = 0.01}
%\end{subfigure}
%
%\end{figure}



\end{document}
