\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{subcaption}
\usepackage{float}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4}    % how many sectioning levels to show in ToC
\begin{document}
\title{\textbf{CS6011: Kernel Methods for Pattern Analysis}
\\
\textbf{Programming Assignment 1}
}
\author{Aravind Sankar CS11B033 \\
Ramnandan SK CS11B061 \\
Adit Krishnan CS11B063 \\[0.2in]
Group 2
}
\floatplacement{figure}{H}
\maketitle
\tableofcontents 
\newpage
\section{Objective of the assignment}
The basic goal of this assignment is to use linear models of regression to do 2 Function Approximation tasks, namely Polynomial Curve fitting on Dataset 1 (univariate) and Surface Fitting using Gaussian Basis functions for Dataset 2 (bivariate). Here, the means of the Gaussians used are the centers of clusters obtained using K-means clustering of the training data. The experimental results obtained are to be summarized using plots obtained on varying different model parameters.
\section{Dataset 1}

\subsection{Generation}
The given function to be approximated is shown below :

\[ f(x) = e^{\cos(2 \pi x)} \]
\[ x \in (0,1) \]

\begin{flushleft}
To generate points from this function, we randomly generated points $x \in (0,1)$ and found the function value $f(x)$ at each point, and added to it Gaussian Noise with mean $\mu = 0$ and standard deviation $\sigma = 0.1$. 3 different training sets were generated, of sizes 20, 100 and 1000. The size of the validation and test set generated were 50 each.
\end{flushleft}

\subsection{Model creation}
\begin{flushleft}
As the goal is to fit a polynomial function to the given data, we need to decide on the degree $M$ of the polynomial which decides the model complexity. 

Through the plots, we show the variation of model outputs and RMS errors on varying different 
parameters and our analysis of the results to choose the best model.

\end{flushleft}
\newpage
\subsection{Results}

\subsubsection{Plots of approximated functions - Question 1}
%\paragraph{Effect of varying $M$}
The following plots showing variation of the polynomial w.r.t degree of polynomial $M$, for training dataset of size $N = 20$. Each plot shows the data points \textcolor{green}{x}, the target function \textcolor{blue}{$f(x)$}  and the approximated polynomial \textcolor{red}{$y(x,\bar{w})$} 
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingM_N20M1}
\caption{N = 20 M = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingM_N20M3}
\caption{N = 20 M = 3}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingM_N20M9}
\caption{N = 20 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingM_N20M15}
\caption{N = 20 M = 15}
\end{subfigure}



\end{figure}
\textbf{Observations :}

\begin{itemize}
\item We chose the training dataset with 20 points to clearly show the overfitting problem (for large model complexities) in the absence of regularization. 
\item Notice that the model with complexity 3 is very close to the true function.
\item For complexities 9 and 15, due to the small number of training data points, we observe overfitting which leads to significant deviation from the underlying target function.

\end{itemize}
\newpage

%\paragraph{Effect of varying $N$}
\begin{flushleft}
The following plots show the different polynomials obtained on varying the training dataset size $N$. Each of these plots are shown for 2 values of $M$, 9 and 15.
The plots for N = 20 and these values of M has already been plotted above. So, the following plots correspond to training dataset sizes of 100 and 1000. 
\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingN_N100M9}
\caption{N = 100 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingN_N100M15}
\caption{N = 100 M = 15}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingN_N1000M9}
\caption{N = 1000 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingN_N1000M15}
\caption{N = 1000 M = 15}
\end{subfigure}



\end{figure}

\textbf{Observations: }
\begin{itemize}
\item We observe that Model complexity $M=9$ almost matches the target function for both datasets since there are sufficient samples.
\item However, Model complexity $M=15$ leads to slight overfitting and hence some deviation from $f(x)$ in case of $N = 100$.
\item However, for the 1000 sample dataset($N = 1000$) both models are almost overlapping with the target function due to the sufficiency of training points.
\item Next we look at the effect of regularization.
\end{itemize}
\newpage

%\paragraph{Effect of varying regularization parameter $\lambda$}
\begin{flushleft}
The following plots show the effects of varying the regularization parameter $\lambda$ on the curve fit. Since the overfitting effect is pronounced only for a small training set, these plots correspond to the 20 point training set and model complexity $M = 9$.
\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda0}
\caption{$\lambda$ = 0}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda01}
\caption{$\lambda$ = 0.1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda1e-05}
\caption{$\lambda$ = $10^{-5}$}
\end{subfigure}



\end{figure}


\textbf{Observations: \newline}
\begin{itemize}
\item We can clearly notice the overfitting when regularization is not applied($\lambda$ = 0) in the first plot.
\item At the same time, $\lambda$ = 1 in the second plot is a very high penalization of weights again leading to a relatively poor fit since less weightage is given to the error on the training points. 
\item The third plot corresponds to $\lambda$ = 0.1. While this leads to a better fit than either of the above, it is still a high value penalization.
\item The last plot corresponds to $\lambda$ = $10^{-5}$. This provides the closest approximation to the true function among these 4 plots. Thus typically small values of $\lambda$ are investigated to minimize the overall error and produce the best fit, using the validation dataset. 
\end{itemize}
\newpage

\subsubsection{Plots of RMS Error - Question 3}
\begin{flushleft}
The following plots show the RMS Error obtained on training models of different complexities and different regularization parameters on training sets of different sizes. For each set of plots generated on varying 1 parameter (say $M$) , the optimal value of the other parameter (say $\lambda$) is obtained by cross-validation. 
The errors have been shown on all 3 datasets for each case (test, train and validation). The plots for 20 size training set undergo lot of variation for every new sampling due to the small size of the dataset, and have been added only for the sake of completion.
\end{flushleft}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_20}
\caption{Varying M, Optimal $\lambda$, 20 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_20}
\caption{Optimal M, Varying $\lambda$, 20 train size}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_100}
\caption{Varying M, Optimal $\lambda$, 100 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_100}
\caption{Optimal M, Varying $\lambda$, 100 train size}
\end{subfigure}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_1000}
\caption{Varying M, Optimal $\lambda$, 1000 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_1000}
\caption{Optimal M, Varying $\lambda$, 1000 train size}
\end{subfigure}

\end{figure}

\subsubsection{Scatter plots of target and model output}
The plots obtained here, have been generated in a process similar to that of question 1. 
The following plots show the scatter plots on the training data obtained on varying the degree of the polynomial $M$ for training dataset size $N = 20$.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M1}
\caption{N = 20 M = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M3}
\caption{N = 20 M = 3}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M9}
\caption{N = 20 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M15}
\caption{N = 20 M = 15}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item In case of a scatter plot, we would ideally like the points to lie along the $y = x$ line. 
\item We observe that for $M=1$, we get a bad fit to the data, which leads to a bad plot.
\item $M = 3$ does better, while $M=9$ and 15 do much better as they fit the training data too well, leading to nearly perfect plots. This is because of overfitting to the training data, which leads to model output being very close to the target output, though the generalization ability is bad.

\end{itemize}

The next set of scatter plots show the variation w.r.t the training set size $N$, namely $N = 100, 1000$.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M9}
\caption{N = 100 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M15}
\caption{N = 100 M = 15}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M9}
\caption{N = 1000 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M15}
\caption{N = 1000 M = 15}
\end{subfigure}



\end{figure}


\textbf{Observations : \newline}
\begin{itemize}
\item For $M=9$, we can see that both of the models ($N = 100 ,1000$) perform well, as we have sufficient number of training examples to build a good model.
\item Again for $M = 15$, we see that both models seem to give good results on the training data. Actually, overfitting happens in the case of $N= 100, M = 15$. To see this, we look at the scatter plots obtained on the test data below for the same model complexity $M = 15$.
\item We can that the generalization ability is more, when we have a greater number of training examples, as exemplified by the better plot obtained when 1000 training examples are used. As we have overfitting when $N = 100, M = 15$, the model output doesn't follow the target output closely. 

\item Next, the effect of $\lambda$ is analyzed.
\end{itemize}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M15_test}
\caption{N = 100 M = 15}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M15_test}
\caption{N = 1000 M = 15}
\end{subfigure}

\end{figure}



The plots below show the scatter plots obtained on varying $\lambda$, for $N = 20$ and $M = 9$. The results obtained were very similar to that of question 1.\newline



\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda0}
\caption{$\lambda$ = 0}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda01}
\caption{$\lambda$ = 0.1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda1e-05}
\caption{$\lambda$ = $10^{-5}$}
\end{subfigure}



\end{figure}

\textbf{Observations :}
\begin{itemize}
\item Clearly, the first plot is the case where $\lambda = 0$, i.e. no regularization.
\item Again for $\lambda = 1$, there is a very high penalization of weights leading to a bad scatter plot, while $\lambda = 0.1$ performs better.

\item The last plot corresponds to $\lambda = 10^{-5}$, which gives a good scatter plot where the model output follows the target output closely and also gives a good generalization on test examples.
\end{itemize}
\newpage
\section{Dataset 2}

\subsection{Generation}
Here, we are given the dataset which is bivariate (2 input variables and 1 output variable). 3 training sets were provided originally, of sizes 20, 100 and 1000. We have additionally sampled training datasets of sizes 200, 500 and 700 for some of the experiments. The test and validation sets are of size 200 and 300 respectively.

\subsection{Model creation}

As the goal is to fit a hyper surface to the given data using the linear model of regression, we need to decide on the number of Gaussian basis functions $M$ which decides the model complexity as well as the centroid and width parameter associated with each Gaussian basis function. 

In order to obtain the centroids of the basis functions, we cluster the training data using $k$-means clustering where $k$ equals the number of basis functions and set the centroids associated with the basis functions to the centroids of the clusters obtained. We set the width parameter of each basis function to be the average distance from the two nearest neighboring cluster centers to the cluster center of this basis function (read from literature).

Through the plots, we show the variation of model outputs and RMS errors on varying different  parameters and our analysis of the results to choose the best model.

Here is a plot of the distribution of the input training data (2000 points) provided (x \& y axes correspond to the 2 input variables and z corresponds to the target output). We observe that the given surface to be approximated has the shape of a bowl.
\begin{figure}[H]
\centering
\includegraphics[width=100mm]{data_scatter}
\caption{Scatter plot of training data input variables}
\label{fig:plot}
\end{figure}



\subsection{Results}


\subsection{Surface Plots of approximated functions - Question 2}
\begin{flushleft}
The Surface plots depict the variation of the surface fit obtained on varying the model complexity ($M$ - No. of basis functions), on the training dataset of size 400. Each plot shows the data points and the fitted surface obtained by linear model of regression on Gaussian Basis functions.

\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingM_N400M10}
\caption{N = 400 M = 10}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingM_N400M50}
\caption{N = 400 M = 50}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingM_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingM_N400M400}
\caption{N = 400 M = 400}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item We chose the training dataset with 400 points to see the shape of surface obtained (for large model complexities) in the absence of regularization. It is to be noted that the maximum model complexity possible here is the dataset size since the number of clusters cannot exceed the number of data points.
\item We notice that for $M=10$, the surface generated has a few peaks around the edges of teh bowl shaped surface, but it's not adequate to fit the data points well.

\item As we increase $M$ to 50 and 100, we observe the emergence of multiple smaller peaks on the surface owing to increase in number of generating Gaussians. These smaller peaks fit the data points well, and we can observe the bowl-shaped surface obtained.
\item Finally, as we increase $M$ to 400, we observe a large number of peaks in the surface which fit the training data more closely leading to the issue of overfitting. The surface obtained here, clearly has a poor generalization capability.
\end{itemize}


\begin{flushleft}
The next set of surface plots show the variation in surface fit on using training datasets of different sizes. Each of these plots are shown for a model complexity of $M = 100$. The training set size is varied as - 100,200,400,700 and 1000. 
%The plot $N =100$ is excluded below, as it has been shown earlier.

\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingN_N200M100}
\caption{N = 200 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingN_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingN_N700M100}
\caption{N = 700 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingN_N1000M100}
\caption{N = 1000 M = 100}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item We have studied that for a given model complexity, the over-fitting problem becomes less severe as the size of the data set increases. This is because the larger the data set, the more complex the model that we can afford to fit to the data.
\item For $M=100$, for smaller data set sizes of $N=200, N=400$, we observe that the peaks are unevenly distributed towards the extremes of the input region and are precisely tuned to fit to the data points in the training data leading to overfitting.
\item As we increase $N$ to 700 and 1000, we observe that the distribution of the peaks becomes more stable and uniform, and we can easily see that the surface fitted would generalize well to new data since the peaks are not fit exactly to few training data points, and it resembles the bowl shaped surface obtained on plotting the target output.
\end{itemize}

\begin{flushleft}
Next, we look at the effect of the regularization parameter $\lambda$ on the surface fits obtained. The plots shown below are for $N = 1000$ and $M = 1000$. The reason we choose $M$ to be high, is to demonstrate the effect of overfitting.


\textbf{SOMEONE DO THESE OBSERVATIONS}


\end{flushleft}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N1000M1000lambda1e-06}
\caption{$\lambda$ = $10^{-6}$}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N1000M1000lambda0_01}
\caption{$\lambda$ = 0.01}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda10}
\caption{$\lambda$ = $10$}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item The very small value of $\lambda=10^{-6}$ almost corresponds to the case with negligible regularization. This corresponds to the case of overfitting as the peaks of the surface are tuned precisely to the training data points and don't generalize well.
\item As the value of $\lambda$ increases upto 10, we penalize the coefficients of the surface generated preventing large weights (in the case of overfitting). Thus, we observe the surface becomes smoother with lesser dominant peaks and these peaks generalize well as they are fit exactly to individual data points.
\end{itemize}


\subsection{Plots of RMS Error - Question 3}

In this section , we show the variation of RMS error, w.r.t model complexity $M$. The plots shown below are for datasets of size 400 and 700.
Again, as earlier, for each set of plots generated on varying 1 parameter (say $M$) , the optimal value of the other parameter (say $\lambda$) is obtained by cross-validation. 
The errors have been shown on all 3 datasets for each case (test, train and validation). 

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/RMS_complexity_400}
\caption{Varying M, Optimal $\lambda$, 400 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/RMS_complexity_700}
\caption{Varying M, Optimal $\lambda$, 700 train size}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/RMS_lambda_400}
\caption{Optimal M, Varying $\lambda$, 400 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/RMS_lambda_700}
\caption{Optimal M, Varying $\lambda$, 700 train size}
\end{subfigure}


\textbf{\\[20pt]Observations :}

\begin{itemize}
\item For the first plot, we notice that there is slight overfitting on the train data with increasing complexity, due to its small size of 400 leading to decrease in RMS error for training data. As we choose the optimal value of $\lambda$ in each case leading to good generalization ability, we see that the RMS error decreases for the test data also. 

\item The second plot closely resembles the first one, but the values of RMS errors obtained are much lower, as the training is done on much larger set of points. Again, we see that RMS error decreases with increase in model complexity.

\item The two bottom plots show the variation in error with regularization.
In both the plots, we observe that the training error is much lesser than the validation and test error. In the first plot, we observe a dip in the value of train error at $\log {\lambda} = -6 $, which denotes the optimal value for $N = 400$. 
\item The second plot also is very similar to the first one. Here, the optimal value of $\log{\lambda}$ occurs at $-4$. We see that a higher value of $\lambda$ is required to get a good fit here, since as the number of data points increases, the optimal model complexity would be higher requiring a higher regularization factor.
\end{itemize}


\end{figure}



\subsection{Scatter plots of target and model output - Question 4}

In these plots, we vary the number of gaussians($M$) with the training dataset fixed at size $N=400$. 
%We choose a reasonably sized training set so that the plots are indicative of overfitting.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N400M10}	
\caption{N = 400 M = 10}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N400M50}
\caption{N = 400 M = 50}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N400M400}
\caption{N = 400 M = 400}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item For a fixed training data set size of 400, we vary the model complexity from 10 to 400 to observe the scatter plot of target output vs model output.
\item For $M= 10$, we have very few gaussian basis functions which leads to the fitted surface being incapable of matching the target output in most cases. 

\item For $M = 50,100$, we see a much better scatter plot, with a lot of points being close to the $y = x$ line.

\item We observe that for the maximum possible $M$ value of 400, we obtain an almost perfect scatter plot as here there are 400 clusters which can generate each training data point separately and hence fit the training data perfectly (i.e huge overfitting, as shown by the plots after this for the corresponding test set).

\end{itemize}
\newpage
Test set plots corresponding to the above training set plots.
\begin{figure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/test_400_400}
\caption{N = 400 M = 400}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/test_1000_400}
\caption{N = 1000 M = 400}
\end{subfigure}
\end{figure}
\textbf{\\[10pt]Remarks:}
\begin{itemize}
\item Clearly for the test set, very poor performance is noticeable for the case of $M=400, N=400$, which performed very well for the training set hence indicating overfitting, while for $N = 1000$, we observe good generalization ability.
\end{itemize}
\newpage
In these plots, we vary the number of points in the training dataset(N) with complexity(M) fixed at 100. 
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N200M100}
\caption{N = 200 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N700M100}
\caption{N = 700 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N1000M100}
\caption{N = 1000 M = 100}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item Moving from $N=200$ to $N=400$ leads to more distortion indicating a more generalizable fit. The same is noticeable as we increase the dataset size even further.
\end{itemize}
\newpage
We now introduce regularization and very the parameter $\lambda$ to inspect its effect on the fitting.

\textbf{SOMEONE DO THESE OBSERVATIONS}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
%\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda1e-06}
\caption{$\lambda$ = $10^{-6}$}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda0_01}
\caption{$\lambda$ = 0.01}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
%\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda10}
\caption{$\lambda$ = $10$}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item In the first plot, which is same as the one previously plotted, the overfit is apparent.
\item As we increase the value of $\lambda$ there is more distortion noticeable.
\end{itemize}



\end{document}
