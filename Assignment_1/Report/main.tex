\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{subcaption}
\usepackage{float}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} % how many sectioning levels to assign numbers to
\setcounter{tocdepth}{4}    % how many sectioning levels to show in ToC
\begin{document}
\title{\textbf{CS6011: Kernel Methods for Pattern Analysis}
\\
\textbf{Programming Assignment 1}
}
\author{Aravind Sankar CS11B033 \\
Ramnandan SK CS11B061 \\
Adit Krishnan CS11B063 \\[0.2in]
Group 2
}
\floatplacement{figure}{H}
\maketitle
\tableofcontents 
\newpage
\section{Objective of the assignment}
The basic goal of this assignment is to use linear models of regression to do 2 Function Approximation tasks, namely Polynomial Curve fitting on Dataset 1 (univariate) and Surface Fitting using Gaussian Basis functions for Dataset 2 (bivariate). Here, the means of the Gaussians used are the centers of clusters obtained using K-means clustering of the training data. The experimental results obtained are to be summarized using plots obtained on varying different model parameters.
\section{Dataset 1}

\subsection{Generation}
The given function to be approximated is shown below :

\[ f(x) = e^{\cos(2 \pi x)} \]
\[ x \in (0,1) \]

\begin{flushleft}
To generate points from this function, we randomly generated points $x \in (0,1)$ and found the function value $f(x)$ at each point, and added to it Gaussian Noise with mean $\mu = 0$ and standard deviation $\sigma = 0.1$. 3 different training sets were generated, of sizes 20, 100 and 1000. The size of the validation and test set generated were 50 each.
\end{flushleft}

\subsection{Model creation}
\begin{flushleft}
As the goal is to fit a polynomial function to the given data, we need to decide on the degree $M$ of the polynomial which decides the model complexity. 

Through the plots, we show the variation of model outputs and RMS errors on varying different 
parameters and our analysis of the results to choose the best model.

\end{flushleft}
\newpage
\subsection{Results}

\subsubsection{Plots of approximated functions - Question 1}
%\paragraph{Effect of varying $M$}
The following plots showing variation of the polynomial w.r.t degree of polynomial $M$, for training dataset of size $N = 20$. Each plot shows the data points \textcolor{green}{x}, the target function \textcolor{blue}{$f(x)$}  and the approximated polynomial \textcolor{red}{$y(x,\bar{w})$} 
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingM_N20M1}
\caption{N = 20 M = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingM_N20M3}
\caption{N = 20 M = 3}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingM_N20M9}
\caption{N = 20 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingM_N20M15}
\caption{N = 20 M = 15}
\end{subfigure}



\end{figure}
\textbf{Observations :}

\begin{itemize}
\item We chose the training dataset with 20 points to clearly show the overfitting problem (for large model complexities) in the absence of regularization. 
\item Notice that the model with complexity 3 is very close to the true function.
\item For complexities 9 and 15, due to the small number of training data points, we observe overfitting which leads to significant deviation from the underlying target function.

\end{itemize}
\newpage

%\paragraph{Effect of varying $N$}
\begin{flushleft}
The following plots show the different polynomials obtained on varying the training dataset size $N$. Each of these plots are shown for 2 values of $M$, 9 and 15.
The plots for N = 20 and these values of M has already been plotted above. So, the following plots correspond to training dataset sizes of 100 and 1000. 
\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingN_N100M9}
\caption{N = 100 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingN_N100M15}
\caption{N = 100 M = 15}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{VaryingN_N1000M9}
\caption{N = 1000 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{VaryingN_N1000M15}
\caption{N = 1000 M = 15}
\end{subfigure}



\end{figure}

\textbf{Observations: }
\begin{itemize}
\item We observe that Model complexity $M=9$ almost matches the target function for both datasets since there are sufficient samples.
\item However, Model complexity $M=15$ leads to slight overfitting and hence some deviation from $f(x)$ in case of $N = 100$.
\item However, for the 1000 sample dataset($N = 1000$) both models are almost overlapping with the target function due to the sufficiency of training points.
\item Next we look at the effect of regularization.
\end{itemize}
\newpage

%\paragraph{Effect of varying regularization parameter $\lambda$}
\begin{flushleft}
The following plots show the effects of varying the regularization parameter $\lambda$ on the curve fit. Since the overfitting effect is pronounced only for a small training set, these plots correspond to the 20 point training set and model complexity $M = 9$.
\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda0}
\caption{$\lambda$ = 0}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda01}
\caption{$\lambda$ = 0.1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Varyinglambda_N20M9lambda1e-05}
\caption{$\lambda$ = $10^{-5}$}
\end{subfigure}



\end{figure}


\textbf{Observations: \newline}
\begin{itemize}
\item We can clearly notice the overfitting when regularization is not applied($\lambda$ = 0) in the first plot.
\item At the same time, $\lambda$ = 1 in the second plot is a very high penalization of weights again leading to a relatively poor fit since less weightage is given to the error on the training points. 
\item The third plot corresponds to $\lambda$ = 0.1. While this leads to a better fit than either of the above, it is still a high value penalization.
\item The last plot corresponds to $\lambda$ = $10^{-5}$. This provides the closest approximation to the true function among these 4 plots. Thus typically small values of $\lambda$ are investigated to minimize the overall error and produce the best fit, using the validation dataset. 
\end{itemize}
\newpage

\subsubsection{Plots of RMS Error - Question 3}
\begin{flushleft}
The following plots show the RMS Error obtained on training models of different complexities and different regularization parameters on training sets of different sizes. For each set of plots generated on varying 1 parameter (say $M$) , the optimal value of the other parameter (say $\lambda$) is obtained by cross-validation. 
The errors have been shown on all 3 datasets for each case (test, train and validation). The plots for 20 size training set undergo lot of variation for every new sampling due to the small size of the dataset, and have been added only for the sake of completion.
\end{flushleft}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_20}
\caption{Varying M, Optimal $\lambda$, 20 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_20}
\caption{Optimal M, Varying $\lambda$, 20 train size}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_100}
\caption{Varying M, Optimal $\lambda$, 100 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_100}
\caption{Optimal M, Varying $\lambda$, 100 train size}
\end{subfigure}
\end{figure}

\begin{figure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{RMS_complexity_1000}
\caption{Varying M, Optimal $\lambda$, 1000 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{RMS_lambda_1000}
\caption{Optimal M, Varying $\lambda$, 1000 train size}
\end{subfigure}

\end{figure}

\subsubsection{Scatter plots of target and model output}
The plots obtained here, have been generated in a process similar to that of question 1. 
The following plots show the scatter plots on the training data obtained on varying the degree of the polynomial $M$ for training dataset size $N = 20$.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M1}
\caption{N = 20 M = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M3}
\caption{N = 20 M = 3}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M9}
\caption{N = 20 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingM_N20M15}
\caption{N = 20 M = 15}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item In case of a scatter plot, we would ideally like the points to lie along the $y = x$ line. 
\item We observe that for $M=1$, we get a bad fit to the data, which leads to a bad plot.
\item $M = 3$ does better, while $M=9$ and 15 do much better as they fit the training data too well, leading to nearly perfect plots. This is because of overfitting to the training data, which leads to model output being very close to the target output, though the generalization ability is bad.

\end{itemize}

The next set of scatter plots show the variation w.r.t the training set size $N$, namely $N = 100, 1000$.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M9}
\caption{N = 100 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M15}
\caption{N = 100 M = 15}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M9}
\caption{N = 1000 M = 9}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M15}
\caption{N = 1000 M = 15}
\end{subfigure}



\end{figure}


\textbf{Observations : \newline}
\begin{itemize}
\item For $M=9$, we can see that both of the models ($N = 100 ,1000$) perform well, as we have sufficient number of training examples to build a good model.
\item Again for $M = 15$, we see that both models seem to give good results on the training data. Actually, overfitting happens in the case of $N= 100, M = 15$. To see this, we look at the scatter plots obtained on the test data below for the same model complexity $M = 15$.
\item We can that the generalization ability is more, when we have a greater number of training examples, as exemplified by the better plot obtained when 1000 training examples are used. As we have overfitting when $N = 100, M = 15$, the model output doesn't follow the target output closely. 

\item Next, the effect of $\lambda$ is analyzed.
\end{itemize}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N100M15_test}
\caption{N = 100 M = 15}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/VaryingN_N1000M15_test}
\caption{N = 1000 M = 15}
\end{subfigure}

\end{figure}



The plots below show the scatter plots obtained on varying $\lambda$, for $N = 20$ and $M = 9$. The results obtained were very similar to that of question 1.\newline



\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda0}
\caption{$\lambda$ = 0}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda01}
\caption{$\lambda$ = 0.1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{Scatter_1/Varyinglambda_N20M9lambda1e-05}
\caption{$\lambda$ = $10^{-5}$}
\end{subfigure}



\end{figure}

\textbf{Observations :}
\begin{itemize}
\item Clearly, the first plot is the case where $\lambda = 0$, i.e. no regularization.
\item Again for $\lambda = 1$, there is a very high penalization of weights leading to a bad scatter plot, while $\lambda = 0.1$ performs better.

\item The last plot corresponds to $\lambda = 10^{-5}$, which gives a good scatter plot where the model output follows the target output closely and also gives a good generalization on test examples.
\end{itemize}
\newpage
\section{Dataset 2}

\subsection{Generation}
Here, we are given the dataset which is bivariate (2 input variables and 1 output variable). 3 training sets were provided originally, of sizes 20, 100 and 1000. We have additionally sampled training datasets of sizes 200, 500 and 700 for some of the experiments. The test and validation sets are of size 200 and 300 respectively.

\subsection{Model creation}

As the goal is to fit a hyper surface to the given data using the linear model of regression, we need to decide on the number of Gaussian basis functions $M$ which decides the model complexity as well as the centroid and width parameter associated with each Gaussian basis function. 

In order to obtain the centroids of the basis functions, we cluster the training data using $k$-means clustering where $k$ equals the number of basis functions and set the centroids associated with the basis functions to the centroids of the clusters obtained. We set the width parameter of each basis function to be the average distance from the two nearest neighboring cluster centers to the cluster centre of this basis function (read from literature).

Through the plots, we show the variation of model outputs and RMS errors on varying different  parameters and our analysis of the results to choose the best model.

Here is a plot of the distribution of the input training data (1000 points) provided (x \& y axes correspond to the 2 input variables). We observe that there is a huge concentration of data points in the range [-10,10] in both input variables.
\begin{figure}[H]
\centering
\includegraphics[width=100mm]{data_scatter}
\caption{Scatter plot of training data input variables}
\label{fig:plot}
\end{figure}



\subsection{Results}


\subsection{Surface Plots of approximated functions - Question 2}
\begin{flushleft}
The Surface plots depict the variation of the surface fit obtained on varying the model complexity ($M$ - No. of basis functions), on the training dataset of size 100. Each plot shows the data points and the fitted surface obtained by linear model of regression on Gaussian Basis functions.

\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingM_N100M10}
\caption{N = 100 M = 10}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingM_N100M40}
\caption{N = 100 M = 40}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingM_N100M80}
\caption{N = 100 M = 80}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingM_N100M100}
\caption{N = 100 M = 100}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item We chose the training dataset with 100 points to clearly show the issue of overfitting (for large model complexities) in the absence of regularization. It is to be noted that the maximum model complexity possible here is the dataset size since the number of clusters cannot exceed the number of data points.
\item We notice that for M=10, the surface generated has a large peak in the region centered about the origin as it has high concentration of data point leading to concentration of most clusters there. 
\item As we increase M to 40, we observe the emergence of multiple smaller peaks on the surface owing to increase in number of generating Gaussians.
\item Finally, as we increase M to 80 and 100, we observe large number of dominant peaks in the surface which fit the training data more closely (leading to the issue of overfitting).
\end{itemize}


\begin{flushleft}
The next set of surface plots show the variation in surface fit on using training datasets of different sizes. Each of these plots are shown for a model complexity of $M = 100$. The training set size is varied as - 100,200,400,700 and 1000. The plot $N =100$ is excluded below, as it has been shown earlier.

\end{flushleft}
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingN_N200M100}
\caption{N = 200 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingN_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/VaryingN_N700M100}
\caption{N = 700 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/VaryingN_N1000M100}
\caption{N = 1000 M = 100}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item We have studied that for a given model complexity, the over-fitting problem becomes less severe as the size of the data set increases. This is because the larger the data set, the more complex the model that we can afford to fit to the data.
\item For M=100, we observe that for smaller data set sizes of N=100, N=200, we observe that the dominant peaks are distributed towards the extremes of the input region and are precisely tuned to fit to the data points in the training data leading to overfitting.
\item As we increase N to 700 and 1000, we observe that the distribution of the peaks becomes more stable and uniform, and it is expected that the surface fitted would generalize well to new data since the peaks are not fit exactly to few training data points.
\end{itemize}

\begin{flushleft}
Next, we look at the effect of the regularization parameter $\lambda$ on the surface fits obtained. The plots shown below are for $N = 100$ and $M = 100$. The reason we choose $M$ to be high, is to demonstrate the effect of overfitting.

\end{flushleft}

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda1e-06}
\caption{$\lambda$ = $10^{-6}$}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda0_01}
\caption{$\lambda$ = 0.01}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Varyinglambda_N100M100lambda10}
\caption{$\lambda$ = $10$}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item The very small value of $\lambda=10^{-6}$ almost corresponds to the case with negligible regularization. This corresponds to the case of overfitting as the peaks of the surface are tuned precisely to the training data points and don't generalize well.
\item As the value of $\lambda$ increases upto 10, we penalize the coefficients of the surface generated preventing large weights (in the case of overfitting). Thus, we observe the surface becomes smoother with lesser dominant peaks and these peaks generalize well as they are fit exactly to individual data points.
\end{itemize}


\subsection{Plots of RMS Error - Question 3}

In this section , we show the variation of RMS error, w.r.t model complexity $M$. The plots shown below are for datasets of size 100 and 1000.
Again, as earlier, for each set of plots generated on varying 1 parameter (say $M$) , the optimal value of the other parameter (say $\lambda$) is obtained by cross-validation. 
The errors have been shown on all 3 datasets for each case (test, train and validation). 

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/RMS_complexity_100}
\caption{Varying M, Optimal $\lambda$, 100 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/RMS_complexity_700}
\caption{Varying M, Optimal $\lambda$, 700 train size}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/RMS_lambda_100}
\caption{Optimal M, Varying $\lambda$, 100 train size}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/RMS_lambda_700}
\caption{Optimal M, Varying $\lambda$, 700 train size}
\end{subfigure}


\textbf{\\[20pt]Observations :}

\begin{itemize}
\item For the first plot, we notice that there is slight overfitting on the train data with increasing complexity, due to its small size of 100 leading to decrease in RMS error. Since $\lambda$ is chosen using the validation set, it shows the least RMS error.
\item  In the second plot we notice moderate overfitting (note that the scale does not begin at 0) on the train data. Even though $\lambda$ is chosen by cross validation, the RMS error on the validation set still grows with increasing complexity of the model since the training points are insufficient leading to overfitting in spite of regularization. The degree of error increase in the validation set (particularly at M $>$ 400 here is more than the previous case due to the high complexity of models being fit.
\item The two bottom plots show the variation in error with regularization. In the first plot the error on training set is spiky and erratic due to its small size. 
\item In the second plot, since M values are chosen to reduce error on the validation set, we see a drop in the same. The test set and train set show relatively higher RMS error and do not seem to vary much with the value of regularization parameter. This is probably because a relatively lower value of complexity gets chosen for higher regularization parameters which seems to perform consistently since it fits only the significant clusters.
\end{itemize}


\end{figure}



\subsection{Scatter plots of target and model output - Question 4}

In these plots, we vary the number of gaussians(M) with the training dataset fixed at size N=100. We choose a relatively small training set so that the plots are indicative of overfitting.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N100M10}
\caption{N = 100 M = 10}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N100M40}
\caption{N = 100 M = 40}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N100M80}
\caption{N = 100 M = 80}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingM_N100M100}
\caption{N = 100 M = 100}
\end{subfigure}


\end{figure}

\textbf{Observations : \newline}
\begin{itemize}
\item For a fixed training data set size of 100, we vary the model complexity from 10 to 100 to observe the scatter plot of model output vs target output.
\item We observe that for the maximum possible M value of 100, we obtain an almost perfect scatter plot (resembles $y=x$ plot) as here there are 100 clusters which can generate each training data point separately and hence fit the training data perfectly (i.e huge overfitting, as shown by the plots after this for the corresponding test set).
\item As we decrease the value of M, the scatter plot gets distorted from $y=x$ line.
\item When we reach M=10, due to the presence of very few Gaussian basis functions, the surface fitted is incapable of matching the target output in most cases.
\end{itemize}
\newpage
Test set plots corresponding to the above training set plots.
\begin{figure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{t1}
\caption{N = 100 M = 80}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{t2}
\caption{N = 100 M = 100}
\end{subfigure}
\end{figure}
\textbf{\\[10pt]Remarks:}
\begin{itemize}
\item Clearly for the test set, very poor performance is noticeable for the case of M=100, N=100, which performed very well for the training set hence indicating overfitting.
\end{itemize}
\newpage
In these plots, we vary the number of points in the training dataset(N) with complexity(M) fixed at 100. 
\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N200M100}
\caption{N = 200 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N400M100}
\caption{N = 400 M = 100}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N700M100}
\caption{N = 700 M = 100}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/VaryingN_N1000M100}
\caption{N = 1000 M = 100}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item Moving from N=100 to N=200 leads to more distortion indicating a more generalizable fit (N=100, M=100 was plotted behind and shows x=y behavior since the model is overfit). The same is noticeable as we increase the dataset size even further.
\end{itemize}
\newpage
We now introduce regularization and very the parameter $\lambda$ to inspect its effect on the fitting.

\begin{figure}[H]

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda1e-06}
\caption{$\lambda$ = $10^{-6}$}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda0_01}
\caption{$\lambda$ = 0.01}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda1}
\caption{$\lambda$ = 1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{D2/Scatter/Varyinglambda_N100M100lambda10}
\caption{$\lambda$ = $10$}
\end{subfigure}



\end{figure}


\textbf{Observations :}

\begin{itemize}
\item In the first plot, which is same as the one previously plotted, the overfit is apparent.
\item As we increase the value of $\lambda$ there is more distortion noticeable.
\end{itemize}



\end{document}
